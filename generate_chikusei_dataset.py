#!/usr/bin/env python3
"""
Chikuseiæ•°æ®é›†ç”Ÿæˆè„šæœ¬
åŸºäºCAVEæ•°æ®é›†çš„å¤„ç†æµç¨‹ï¼Œé€‚é…Chikuseiæ•°æ®é›†çš„ç‰¹æ®Šè£åˆ‡ç­–ç•¥

æ•°æ®æµç¨‹:
1. GT (Ground Truth) - åŸå§‹128é€šé“é«˜å…‰è°±æ•°æ®
2. HRMSI - 3é€šé“RGBæ•°æ®  
3. LRHSI_X - GTé€šè¿‡Interp23TapæŠ—æ··å ä¸‹é‡‡æ ·Xå€å¾—åˆ°çš„ä½åˆ†è¾¨ç‡é«˜å…‰è°±æ•°æ®
4. lms_X - LRHSI_Xé€šè¿‡åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·å›åŸå°ºå¯¸å¾—åˆ°çš„ä½åˆ†è¾¨ç‡å¤šå…‰è°±æ•°æ®

è£åˆ‡ç­–ç•¥:
- è®­ç»ƒé›†: æŒ‰æ­¥é•¿é‡å è£åˆ‡å‡ºè‹¥å¹²ä¸ª128x128çš„patch
- æµ‹è¯•é›†: ä»å·¦ä¸Šè§’2048x2048åŒºåŸŸè£å‡ºä¸é‡å çš„1024x1024çš„å››å¼ 

ä½œè€…: Assistant  
æ—¥æœŸ: 2025-08-10
"""

import os
import sys
import numpy as np
import h5py
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import math
from pathlib import Path
import scipy.io as sio

# é…ç½®å‚æ•°
DEFAULT_CONFIG = {
    'chikusei_file': '/data2/users/yujieliang/dataset/Chikusei/Chikusei_ROSIS_RGB.h5',
    'output_dir': '/data2/users/yujieliang/dataset/Chikusei',
    'patch_size': 128,           # è®­ç»ƒpatchå°ºå¯¸
    'test_patch_size': 1024,     # æµ‹è¯•patchå°ºå¯¸  
    'test_region_size': 2048,    # æµ‹è¯•åŒºåŸŸå°ºå¯¸
    'stride': 32,                # è®­ç»ƒpatchæ­¥é•¿
    'downsample_factors': [4, 8, 16, 32],
    'compression_level': 9,
    'rgb_bands': [29, 19, 9],    # Chikuseiçš„RGBæ³¢æ®µç´¢å¼• (R, G, B)
}

class Interp23Tap(nn.Module):
    """
    PyTorch implementation of the interp23tap MATLAB function.
    """

    def __init__(self, ratio: int, pad_mode: str = "replicate"):
        super().__init__()

        if not (ratio > 0 and (ratio & (ratio - 1) == 0)):
            raise ValueError("Error: Only resize factors power of 2 are supported.")
        self.ratio = ratio
        self.num_upsamples = int(math.log2(ratio))
        self.pad_mode = pad_mode

        # Define the 23-tap filter coefficients (CDF23 from MATLAB code)
        cdf23_coeffs = 2.0 * np.array(
            [
                0.5,
                0.305334091185,
                0.0,
                -0.072698593239,
                0.0,
                0.021809577942,
                0.0,
                -0.005192756653,
                0.0,
                0.000807762146,
                0.0,
                -0.000060081482,
            ]
        )
        # Make symmetric
        base_coeffs = np.concatenate([np.flip(cdf23_coeffs[1:]), cdf23_coeffs])
        base_coeffs_t = torch.tensor(base_coeffs, dtype=torch.float32)

        # Reshape kernel for 2D convolution (separable filter)
        kernel_h = base_coeffs_t.view(1, 1, -1, 1)  # Shape (1, 1, 23, 1)
        kernel_w = base_coeffs_t.view(1, 1, 1, -1)  # Shape (1, 1, 1, 23)

        # Register kernels as buffers
        self.register_buffer("kernel_h", kernel_h)
        self.register_buffer("kernel_w", kernel_w)

        # Calculate padding size (kernel_size=23)
        self.padding = (base_coeffs_t.shape[0] - 1) // 2  # Should be 11

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.ratio == 1:
            return x

        current_img = x
        bs, c, h_curr, w_curr = current_img.shape

        for k in range(self.num_upsamples):
            h_curr *= 2
            w_curr *= 2

            # Upsample by inserting zeros
            upsampled = torch.zeros(
                bs, c, h_curr, w_curr, device=x.device, dtype=x.dtype
            )

            # Place original pixels according to MATLAB logic
            if k == 0:
                upsampled[..., 1::2, 1::2] = current_img
            else:
                upsampled[..., ::2, ::2] = current_img

            # Apply separable convolution with circular padding
            # Pad for horizontal filter (width)
            padded_w = F.pad(
                upsampled, (self.padding, self.padding, 0, 0), mode=self.pad_mode
            )
            kernel_w_grouped = self.kernel_w.repeat(c, 1, 1, 1)
            filtered_w = F.conv2d(padded_w, kernel_w_grouped, groups=c)

            # Pad for vertical filter (height)
            padded_h = F.pad(
                filtered_w, (0, 0, self.padding, self.padding), mode="circular"
            )
            kernel_h_grouped = self.kernel_h.repeat(c, 1, 1, 1)
            filtered_h = F.conv2d(padded_h, kernel_h_grouped, groups=c)

            current_img = filtered_h

        return current_img


def anti_aliasing_downsample(image_tensor, factor, device):
    """ä½¿ç”¨Interp23Tapè¿›è¡ŒæŠ—æ··å ä¸‹é‡‡æ ·"""
    if factor == 1:
        return image_tensor
    
    # æ·»åŠ batchç»´åº¦
    img_batch = image_tensor.unsqueeze(0).to(device)  # (1, C, H, W)
    
    # ä½¿ç”¨Interp23Tapçš„æ»¤æ³¢æ ¸è¿›è¡ŒæŠ—æ··å é¢„å¤„ç†
    cdf23_coeffs = 2.0 * np.array([
        0.5, 0.305334091185, 0.0, -0.072698593239, 0.0, 0.021809577942,
        0.0, -0.005192756653, 0.0, 0.000807762146, 0.0, -0.000060081482,
    ])
    base_coeffs = np.concatenate([np.flip(cdf23_coeffs[1:]), cdf23_coeffs])
    base_coeffs_t = torch.tensor(base_coeffs, dtype=torch.float32, device=device)
    
    # åˆ›å»ºåˆ†ç¦»æ»¤æ³¢æ ¸
    kernel_h = base_coeffs_t.view(1, 1, -1, 1)
    kernel_w = base_coeffs_t.view(1, 1, 1, -1)
    padding = (len(base_coeffs) - 1) // 2
    
    # åº”ç”¨æŠ—æ··å æ»¤æ³¢
    bs, c, h, w = img_batch.shape
    
    # æ°´å¹³æ»¤æ³¢
    padded_w = F.pad(img_batch, (padding, padding, 0, 0), mode='replicate')
    kernel_w_grouped = kernel_w.repeat(c, 1, 1, 1)
    filtered_w = F.conv2d(padded_w, kernel_w_grouped, groups=c)
    
    # å‚ç›´æ»¤æ³¢
    padded_h = F.pad(filtered_w, (0, 0, padding, padding), mode='replicate')
    kernel_h_grouped = kernel_h.repeat(c, 1, 1, 1)
    filtered_h = F.conv2d(padded_h, kernel_h_grouped, groups=c)
    
    # ä¸‹é‡‡æ ·ï¼ˆå­é‡‡æ ·ï¼‰
    downsampled = filtered_h[:, :, ::factor, ::factor]
    
    return downsampled.squeeze(0)  # ç§»é™¤batchç»´åº¦


class ChikuseiDatasetGenerator:
    """Chikuseiæ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = config or DEFAULT_CONFIG.copy()
        self.device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
    def load_chikusei_data(self):
        """åŠ è½½åŸå§‹Chikuseiæ•°æ®"""
        print("ğŸ“‚ åŠ è½½åŸå§‹Chikuseiæ•°æ®...")
        
        chikusei_file = self.config['chikusei_file']
        if not os.path.exists(chikusei_file):
            raise FileNotFoundError(f"Chikuseiæ–‡ä»¶ä¸å­˜åœ¨: {chikusei_file}")
        
        # åŠ è½½H5æ–‡ä»¶
        print(f"   ä» {chikusei_file} åŠ è½½æ•°æ®...")
        
        with h5py.File(chikusei_file, 'r') as f:
            print(f"   H5æ–‡ä»¶é”®å€¼: {list(f.keys())}")
            
            # åŠ è½½GTæ•°æ® (128é€šé“é«˜å…‰è°±)
            if 'gt' not in f:
                raise ValueError("æ–‡ä»¶ä¸­ç¼ºå°‘'gt'æ•°æ®é›†")
            
            self.gt_data = f['gt'][:].astype(np.float32)  # (128, 2335, 2517)
            print(f"   GTæ•°æ®å½¢çŠ¶: {self.gt_data.shape}")
            print(f"   GTæ•°å€¼èŒƒå›´: [{self.gt_data.min():.2f}, {self.gt_data.max():.2f}]")
            
            # åŠ è½½HRMSIæ•°æ® (3é€šé“RGB)
            if 'HRMSI' not in f:
                raise ValueError("æ–‡ä»¶ä¸­ç¼ºå°‘'HRMSI'æ•°æ®é›†")
            
            self.hrmsi_data = f['HRMSI'][:].astype(np.float32)  # (3, 2335, 2517)
            print(f"   HRMSIæ•°æ®å½¢çŠ¶: {self.hrmsi_data.shape}")
            print(f"   HRMSIæ•°å€¼èŒƒå›´: [{self.hrmsi_data.min():.2f}, {self.hrmsi_data.max():.2f}]")
            
        
        # # æ•°æ®å½’ä¸€åŒ–åˆ° [0, 1]
        # if self.gt_data.max() > 1.0:
        #     print(f"   GTæ•°æ®å½’ä¸€åŒ–: [{self.gt_data.min()}, {self.gt_data.max()}] -> [0, 1]")
        #     self.gt_data = (self.gt_data - self.gt_data.min()) / (self.gt_data.max() - self.gt_data.min())
        
        # if self.hrmsi_data.max() > 1.0:
        #     print(f"   HRMSIæ•°æ®å½’ä¸€åŒ–: [{self.hrmsi_data.min()}, {self.hrmsi_data.max()}] -> [0, 1]")
        #     self.hrmsi_data = (self.hrmsi_data - self.hrmsi_data.min()) / (self.hrmsi_data.max() - self.hrmsi_data.min())
        
        print(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ!")
        print(f"   æœ€ç»ˆGTæ•°æ®: {self.gt_data.shape} (CÃ—HÃ—W)")
        print(f"   æœ€ç»ˆHRMSIæ•°æ®: {self.hrmsi_data.shape} (CÃ—HÃ—W)")
        
    def crop_training_patches(self):
        """è£å‰ªè®­ç»ƒpatches - é‡å è£åˆ‡"""
        print("âœ‚ï¸ è£å‰ªè®­ç»ƒpatches (é‡å è£åˆ‡)...")
        
        patch_size = self.config['patch_size']
        stride = self.config['stride']
        
        c, h, w = self.gt_data.shape
        
        # è®¡ç®—å¯è£å‰ªçš„patchæ•°é‡
        n_patches_h = (h - patch_size) // stride + 1
        n_patches_w = (w - patch_size) // stride + 1
        total_patches = n_patches_h * n_patches_w
        
        print(f"   åŸå§‹å›¾åƒå°ºå¯¸: {h} Ã— {w}")
        print(f"   Patchå°ºå¯¸: {patch_size} Ã— {patch_size}")
        print(f"   æ­¥é•¿: {stride}")
        print(f"   é¢„è®¡patchæ•°é‡: {total_patches}")
        
        # è£å‰ªpatches
        gt_patches = []
        hrmsi_patches = []
        
        for i in tqdm(range(0, h - patch_size + 1, stride), desc="è£å‰ªrows"):
            for j in range(0, w - patch_size + 1, stride):
                # GT patch
                gt_patch = self.gt_data[:, i:i+patch_size, j:j+patch_size]  # (C, patch_size, patch_size)
                gt_patches.append(gt_patch)
                
                # HRMSI patch
                hrmsi_patch = self.hrmsi_data[:, i:i+patch_size, j:j+patch_size]  # (3, patch_size, patch_size)
                hrmsi_patches.append(hrmsi_patch)
        
        print(f"   å®é™…ç”Ÿæˆpatchæ•°é‡: {len(gt_patches)}")
        
        return gt_patches, hrmsi_patches
    
    def crop_test_patches(self):
        """è£å‰ªæµ‹è¯•patches - ä»å·¦ä¸Šè§’2048x2048åŒºåŸŸè£å‡ºä¸é‡å çš„1024x1024å››å¼ """
        print("âœ‚ï¸ è£å‰ªæµ‹è¯•patches (2048x2048åŒºåŸŸçš„å››ä¸ª1024x1024)...")
        
        test_region_size = self.config['test_region_size']
        test_patch_size = self.config['test_patch_size']
        
        c, h, w = self.gt_data.shape
        
        # æ£€æŸ¥å°ºå¯¸
        if h < test_region_size or w < test_region_size:
            raise ValueError(f"å›¾åƒå°ºå¯¸ {h}Ã—{w} å°äºæµ‹è¯•åŒºåŸŸå°ºå¯¸ {test_region_size}Ã—{test_region_size}")
        
        print(f"   æµ‹è¯•åŒºåŸŸ: å·¦ä¸Šè§’ {test_region_size} Ã— {test_region_size}")
        print(f"   æµ‹è¯•patchå°ºå¯¸: {test_patch_size} Ã— {test_patch_size}")
        
        # æå–æµ‹è¯•åŒºåŸŸ
        test_region_gt = self.gt_data[:, :test_region_size, :test_region_size]
        test_region_hrmsi = self.hrmsi_data[:, :test_region_size, :test_region_size]
        
        # è£å‰ªå››ä¸ªä¸é‡å çš„patches
        gt_patches = []
        hrmsi_patches = []
        
        # å››ä¸ªpatchçš„å·¦ä¸Šè§’åæ ‡
        positions = [
            (0, 0),                                    # å·¦ä¸Š
            (0, test_patch_size),                      # å³ä¸Š  
            (test_patch_size, 0),                      # å·¦ä¸‹
            (test_patch_size, test_patch_size)         # å³ä¸‹
        ]
        
        for i, (start_h, start_w) in enumerate(positions):
            end_h = start_h + test_patch_size
            end_w = start_w + test_patch_size
            
            gt_patch = test_region_gt[:, start_h:end_h, start_w:end_w]
            hrmsi_patch = test_region_hrmsi[:, start_h:end_h, start_w:end_w]
            
            gt_patches.append(gt_patch)
            hrmsi_patches.append(hrmsi_patch)
            
            print(f"   Patch {i+1}: [{start_h}:{end_h}, {start_w}:{end_w}] -> {gt_patch.shape}")
        
        return gt_patches, hrmsi_patches
    
    def generate_training_set(self):
        """ç”Ÿæˆè®­ç»ƒé›† - è¾¹å¤„ç†è¾¹å­˜å‚¨ç‰ˆæœ¬"""
        print("\n" + "="*60)
        print("1ï¸âƒ£ ç”ŸæˆChikuseiè®­ç»ƒé›†ï¼ˆè¾¹å¤„ç†è¾¹å­˜å‚¨ï¼‰")
        print("="*60)
        
        patch_size = self.config['patch_size']
        stride = self.config['stride']
        downsample_factors = self.config['downsample_factors']
        
        # ä¼°ç®—patchesæ•°é‡
        c, h, w = self.gt_data.shape
        n_patches_h = (h - patch_size) // stride + 1
        n_patches_w = (w - patch_size) // stride + 1
        total_estimated_patches = n_patches_h * n_patches_w
        
        print(f"åŸå§‹å›¾åƒå°ºå¯¸: {h} Ã— {w}")
        print(f"ä¼°è®¡patchesæ•°é‡: {total_estimated_patches}")
        
        # é¦–å…ˆæ‰¹é‡è£å‰ªæ‰€æœ‰patches
        print("\nâœ‚ï¸ æ‰¹é‡è£å‰ªpatches...")
        gt_patches = []
        hrmsi_patches = []
        
        for i in tqdm(range(0, h - patch_size + 1, stride), desc="è£å‰ªpatches"):
            for j in range(0, w - patch_size + 1, stride):
                gt_patch = self.gt_data[:, i:i+patch_size, j:j+patch_size]
                hrmsi_patch = self.hrmsi_data[:, i:i+patch_size, j:j+patch_size]
                
                gt_patches.append(gt_patch)
                hrmsi_patches.append(hrmsi_patch)
        
        print(f"å®é™…patchesæ•°é‡: {len(gt_patches)}")
        
        # åˆ›å»ºè¾“å‡ºH5æ–‡ä»¶
        train_file = os.path.join(
            self.config['output_dir'], 
            f'Chikusei_train_patches_stride{stride}_size{patch_size}.h5'
        )
        
        # è¾¹å¤„ç†è¾¹å­˜å‚¨
        with h5py.File(train_file, 'w') as f:
            compression_opts = self.config['compression_level']
            
            # é¦–å…ˆä¿å­˜GTå’ŒHRMSIï¼ˆè¿™äº›å·²ç»å‡†å¤‡å¥½äº†ï¼‰
            print("\nğŸ’¾ ä¿å­˜GTå’ŒHRMSI...")
            gt_stack = np.stack(gt_patches)
            hrmsi_stack = np.stack(hrmsi_patches)
            
            f.create_dataset('GT', data=gt_stack,
                            #   compression='gzip', compression_opts=compression_opts
                              )
            f.create_dataset('HRMSI', data=hrmsi_stack,
                            #   compression='gzip', compression_opts=compression_opts
                              )
            
            # é‡Šæ”¾å†…å­˜
            del gt_stack, hrmsi_stack
            import gc
            gc.collect()
            
            # é€ä¸ªå€ç‡å¤„ç†LRHSIå’ŒLMS
            for factor in downsample_factors:
                print(f"\nğŸ”„ å¤„ç†ä¸‹é‡‡æ ·å€æ•° {factor}x...")
                
                # å•ç‹¬å¤„ç†å½“å‰å€ç‡
                lrhsi_data, lms_data = self.generate_single_factor_data(
                    gt_patches, factor, (patch_size, patch_size)
                )
                
                # ç«‹å³ä¿å­˜å½“å‰å€ç‡çš„æ•°æ®
                print(f"ğŸ’¾ ä¿å­˜ {factor}x å€ç‡æ•°æ®...")
                f.create_dataset(f'LRHSI_{factor}', 
                               data=np.stack(lrhsi_data), 
                            #    compression='gzip', compression_opts=compression_opts
                               )
                f.create_dataset(f'lms_{factor}', 
                               data=np.stack(lms_data), 
                            #    compression='gzip', compression_opts=compression_opts
                               )
                
                # ç«‹å³é‡Šæ”¾å†…å­˜
                del lrhsi_data, lms_data
                gc.collect()
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                print(f"âœ… {factor}x å€ç‡å¤„ç†å®Œæˆ")
            
            # ä¿å­˜å…ƒæ•°æ®
            f.attrs['patch_size'] = patch_size
            f.attrs['stride'] = stride
            f.attrs['total_patches'] = len(gt_patches)
            f.attrs['downsample_factors'] = downsample_factors
            f.attrs['original_image_shape'] = self.gt_data.shape
    
        file_size_mb = os.path.getsize(train_file) / (1024**2)
        print(f"\nâœ… è®­ç»ƒé›†ä¿å­˜å®Œæˆ:")
        print(f"   æ–‡ä»¶: {train_file}")
        print(f"   å¤§å°: {file_size_mb:.1f} MB")
        print(f"   å®é™…patches: {len(gt_patches)}")
        
        return train_file

    def generate_single_factor_data(self, images, factor, original_size):
        """
        ç”Ÿæˆå•ä¸ªä¸‹é‡‡æ ·å€ç‡çš„LRHSIå’ŒLMSæ•°æ®
        """
        lrhsi_data = []
        lms_data = []
        
        print(f"   æ€»patchesæ•°é‡: {len(images)}")
        
        # å‡å°æ‰¹å¤„ç†å¤§å°ä»¥èŠ‚çœå†…å­˜
        batch_size = 64 if self.device.type == 'cuda' else 32
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(images), batch_size), desc=f"å¤„ç†{factor}xä¸‹é‡‡æ ·"):
            batch_images = images[i:i+batch_size]
            
            try:
                # æ‰¹é‡å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_lrhsi = []
                batch_lms = []
                
                # è¿›ä¸€æ­¥å‡å°å­æ‰¹æ¬¡å¤§å°
                sub_batch_size = 16
                for j in range(0, len(batch_images), sub_batch_size):
                    sub_batch = batch_images[j:j+sub_batch_size]
                    
                    for image in sub_batch:
                        # è½¬æ¢ä¸ºtensor
                        if isinstance(image, np.ndarray):
                            img_tensor = torch.from_numpy(image).float()
                        else:
                            img_tensor = image
                        
                        # ä¸‹é‡‡æ ·
                        lrhsi_tensor = anti_aliasing_downsample(img_tensor, factor, self.device)
                        lrhsi = lrhsi_tensor.cpu().numpy()
                        batch_lrhsi.append(lrhsi)
                        
                        # ä¸Šé‡‡æ ·ç”ŸæˆLMS
                        lrhsi_tensor = lrhsi_tensor.to(self.device)
                        lrhsi_batch = lrhsi_tensor.unsqueeze(0)
                        lms_tensor = F.interpolate(lrhsi_batch, size=original_size, 
                                                 mode='bilinear', align_corners=False)
                        lms = lms_tensor.squeeze(0).cpu().numpy()
                        batch_lms.append(lms)
                        
                        # ç«‹å³æ¸…ç†GPUå†…å­˜
                        del img_tensor, lrhsi_tensor, lrhsi_batch, lms_tensor
                        if self.device.type == 'cuda':
                            torch.cuda.empty_cache()
                
                # ä¿å­˜æ‰¹æ¬¡ç»“æœ
                lrhsi_data.extend(batch_lrhsi)
                lms_data.extend(batch_lms)
                
                # æ¸…ç†æ‰¹æ¬¡å†…å­˜
                del batch_lrhsi, batch_lms
                
            except Exception as e:
                print(f"æ‰¹å¤„ç†å¤±è´¥ (factor={factor}, batch {i//batch_size}): {e}")
                # é™çº§å¤„ç†
                for image in batch_images:
                    try:
                        if factor > 1:
                            lrhsi = image[:, ::factor, ::factor]
                        else:
                            lrhsi = image
                        lrhsi_data.append(lrhsi)
                        
                        # ç®€å•ä¸Šé‡‡æ ·
                        if original_size is not None and factor > 1:
                            from scipy.ndimage import zoom
                            zoom_factors = (1, original_size[0]/lrhsi.shape[-2], original_size[1]/lrhsi.shape[-1])
                            lms = zoom(lrhsi, zoom_factors, order=1)
                        else:
                            lms = lrhsi
                        lms_data.append(lms)
                    except Exception as e2:
                        print(f"é™çº§å¤„ç†å¤±è´¥: {e2}")
        
        # æ¯æ‰¹å¤„ç†å®Œå¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
    
        return lrhsi_data, lms_data
    
    def generate_test_set(self):
        """ç”Ÿæˆæµ‹è¯•é›† - è¾¹å¤„ç†è¾¹å­˜å‚¨ç‰ˆæœ¬"""
        print("\n" + "="*60)
        print("2ï¸âƒ£ ç”Ÿæˆæµ‹è¯•é›†ï¼ˆè¾¹å¤„ç†è¾¹å­˜å‚¨ï¼‰")
        print("="*60)
        
        # è£å‰ªæµ‹è¯•patches
        gt_patches, hrmsi_patches = self.crop_test_patches()
        
        # åˆ›å»ºè¾“å‡ºH5æ–‡ä»¶
        test_file = os.path.join(self.config['output_dir'], 'Chikusei_test_patches.h5')
        
        with h5py.File(test_file, 'w') as f:
            compression_opts = self.config['compression_level']
            
            # é¦–å…ˆä¿å­˜GTå’ŒHRMSI
            print("\nğŸ’¾ ä¿å­˜æµ‹è¯•é›†GTå’ŒHRMSI...")
            gt_stack = np.stack(gt_patches)
            hrmsi_stack = np.stack(hrmsi_patches)
            
            f.create_dataset('GT', data=gt_stack)
            f.create_dataset('HRMSI', data=hrmsi_stack)
            
            # é‡Šæ”¾å†…å­˜
            del gt_stack, hrmsi_stack
            import gc
            gc.collect()
            
            # é€ä¸ªå€ç‡å¤„ç†
            downsample_factors = self.config['downsample_factors']
            test_patch_size = self.config['test_patch_size']
            
            for factor in downsample_factors:
                print(f"\nğŸ”„ å¤„ç†æµ‹è¯•é›† {factor}x å€ç‡...")
                
                # å•ç‹¬å¤„ç†å½“å‰å€ç‡
                lrhsi_data, lms_data = self.generate_single_factor_data(
                    gt_patches, factor, (test_patch_size, test_patch_size)
                )
                
                # ç«‹å³ä¿å­˜
                print(f"ğŸ’¾ ä¿å­˜æµ‹è¯•é›† {factor}x å€ç‡æ•°æ®...")
                f.create_dataset(f'LRHSI_{factor}', data=np.stack(lrhsi_data))
                f.create_dataset(f'lms_{factor}', data=np.stack(lms_data))
                
                # ç«‹å³é‡Šæ”¾å†…å­˜
                del lrhsi_data, lms_data
                gc.collect()
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                print(f"âœ… æµ‹è¯•é›† {factor}x å€ç‡å¤„ç†å®Œæˆ")
            
            # ä¿å­˜å…ƒæ•°æ®
            f.attrs['test_patch_size'] = test_patch_size
            f.attrs['test_region_size'] = self.config['test_region_size']
            f.attrs['total_test_patches'] = len(gt_patches)
            f.attrs['downsample_factors'] = downsample_factors
            f.attrs['original_image_shape'] = self.gt_data.shape
    
        file_size_mb = os.path.getsize(test_file) / (1024**2)
        print(f"\nâœ… æµ‹è¯•é›†ä¿å­˜å®Œæˆ:")
        print(f"   æ–‡ä»¶: {test_file}")
        print(f"   å¤§å°: {file_size_mb:.1f} MB")
        
        return test_file
    
    def verify_datasets(self, train_file, test_file):
        """éªŒè¯ç”Ÿæˆçš„æ•°æ®é›†"""
        print("\n" + "="*60)
        print("3ï¸âƒ£ éªŒè¯æ•°æ®é›†")
        print("="*60)
        
        # éªŒè¯è®­ç»ƒé›†
        print("ğŸ” éªŒè¯è®­ç»ƒé›†:")
        with h5py.File(train_file, 'r') as f:
            print(f"   æ•°æ®é›†é”®å€¼: {list(f.keys())}")
            for key in ['GT', 'HRMSI'] + [f'LRHSI_{factor}' for factor in self.config['downsample_factors']] + [f'lms_{factor}' for factor in self.config['downsample_factors']]:
                if key in f:
                    data = f[key]
                    print(f"   {key}: {data.shape}, dtype={data.dtype}")
            print(f"   å±æ€§: {dict(f.attrs)}")
        
        # éªŒè¯æµ‹è¯•é›†
        print("\nğŸ” éªŒè¯æµ‹è¯•é›†:")
        with h5py.File(test_file, 'r') as f:
            print(f"   æ•°æ®é›†é”®å€¼: {list(f.keys())}")
            for key in ['GT', 'HRMSI'] + [f'LRHSI_{factor}' for factor in self.config['downsample_factors']] + [f'lms_{factor}' for factor in self.config['downsample_factors']]:
                if key in f:
                    data = f[key]
                    print(f"   {key}: {data.shape}, dtype={data.dtype}")
            print(f"   å±æ€§: {dict(f.attrs)}")
        
        print("\nâœ… æ•°æ®é›†éªŒè¯å®Œæˆ!")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹ç”ŸæˆChikuseiæ•°æ®é›†")
        print("="*80)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_chikusei_data()
            
            # 2. ç”Ÿæˆè®­ç»ƒé›†
            train_file = self.generate_training_set()
            
            # 3. ç”Ÿæˆæµ‹è¯•é›†
            test_file = self.generate_test_set()
            
            # 4. éªŒè¯æ•°æ®é›†
            self.verify_datasets(train_file, test_file)
            
            # 5. æ€»ç»“
            print("\n" + "="*80)
            print("ğŸ‰ Chikuseiæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
            print("="*80)
            print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            print(f"   è®­ç»ƒé›†: {train_file}")
            print(f"   æµ‹è¯•é›†: {test_file}")
            print()
            print("âœ… æ•°æ®é›†ç‰¹ç‚¹:")
            print("   â€¢ GT: 128é€šé“é«˜å…‰è°±æ•°æ®")
            print("   â€¢ HRMSI: 3é€šé“RGBæ•°æ®")
            print("   â€¢ LRHSI_X: ä½¿ç”¨Interp23TapæŠ—æ··å ä¸‹é‡‡æ ·çš„ä½åˆ†è¾¨ç‡é«˜å…‰è°±æ•°æ®")
            print("   â€¢ lms_X: ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·å›åŸå°ºå¯¸çš„ä½åˆ†è¾¨ç‡å¤šå…‰è°±æ•°æ®")
            print("   â€¢ è®­ç»ƒé›†: 128Ã—128 patchesï¼Œé‡å è£åˆ‡")
            print("   â€¢ æµ‹è¯•é›†: å››ä¸ª1024Ã—1024 patchesï¼Œä»å·¦ä¸Šè§’2048Ã—2048åŒºåŸŸä¸é‡å è£åˆ‡")
            print("="*80)
            
            return train_file, test_file
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Chikuseiæ•°æ®é›†ç”Ÿæˆå™¨')
    
    parser.add_argument('--chikusei_file', type=str, 
                       default=DEFAULT_CONFIG['chikusei_file'],
                       help='ChikuseiåŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--output_dir', type=str,
                       default=DEFAULT_CONFIG['output_dir'],
                       help='è¾“å‡ºç›®å½•')
    
    parser.add_argument('--patch_size', type=int,
                       default=DEFAULT_CONFIG['patch_size'],
                       help='è®­ç»ƒpatchå°ºå¯¸')
    
    parser.add_argument('--test_patch_size', type=int,
                       default=DEFAULT_CONFIG['test_patch_size'],
                       help='æµ‹è¯•patchå°ºå¯¸')
    
    parser.add_argument('--stride', type=int,
                       default=DEFAULT_CONFIG['stride'],
                       help='è®­ç»ƒpatchæ­¥é•¿')
    
    parser.add_argument('--downsample_factors', nargs='+', type=int,
                       default=DEFAULT_CONFIG['downsample_factors'],
                       help='ä¸‹é‡‡æ ·å€æ•°åˆ—è¡¨')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # åˆ›å»ºé…ç½®
    config = DEFAULT_CONFIG.copy()
    config.update(vars(args))
    
    print("é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è¿è¡Œ
    generator = ChikuseiDatasetGenerator(config)
    train_file, test_file = generator.run()
    
    if train_file and test_file:
        print(f"\nâœ… æ•°æ®é›†ç”ŸæˆæˆåŠŸ!")
        print(f"å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½æ•°æ®:")
        print(f"")
        print(f"import h5py")
        print(f"with h5py.File('{train_file}', 'r') as f:")
        print(f"    gt = f['GT'][:]")
        print(f"    lrhsi_4 = f['LRHSI_4'][:]")
        print(f"    lms_4 = f['lms_4'][:]")
    else:
        print(f"\nâŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥!")
        sys.exit(1)


if __name__ == '__main__':
    main()