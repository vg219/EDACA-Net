#!/usr/bin/env python3
"""
Harvardæ•°æ®é›†ç”Ÿæˆè„šæœ¬
ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ŒåŒ…å«GT, HRMSI, LRHSI_X, lms_Xæ•°æ®

æ•°æ®æµç¨‹:
1. GT (Ground Truth) - åŸå§‹31é€šé“é«˜å…‰è°±æ•°æ®
2. HRMSI - 3é€šé“RGBæ•°æ®
3. LRHSI_X - GTé€šè¿‡Interp23TapæŠ—æ··å ä¸‹é‡‡æ ·Xå€å¾—åˆ°çš„ä½åˆ†è¾¨ç‡é«˜å…‰è°±æ•°æ®
4. lms_X - LRHSI_Xé€šè¿‡åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·å›åŸå°ºå¯¸å¾—åˆ°çš„ä½åˆ†è¾¨ç‡å¤šå…‰è°±æ•°æ®

è®­ç»ƒé›†ï¼š67å¼ å›¾åƒï¼Œ64æ­¥é•¿è£åˆ‡128x128 patches
æµ‹è¯•é›†ï¼š10å¼ å›¾åƒï¼Œå·¦ä¸Šè§’1024x1024è£åˆ‡

ä½œè€…: Assistant
æ—¥æœŸ: 2025-08-05
"""

import os
import sys
import numpy as np
import h5py
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import math
from pathlib import Path

# é…ç½®å‚æ•°
DEFAULT_CONFIG = {
    'harvard_file': '/data2/users/yujieliang/dataset/Harvard_calibrated_full.h5',
    'output_dir': '/data2/users/yujieliang/dataset/Harvard',
    'patch_size': 128,
    'stride': 128,
    'test_crop_size': 1024,  # æµ‹è¯•é›†è£åˆ‡å°ºå¯¸
    'downsample_factors': [4, 8, 16, 32],
    'train_indices': list(range(67)),      # å‰67å¼ ä½œä¸ºè®­ç»ƒé›†
    'test_indices': list(range(67, 77)),   # å10å¼ ä½œä¸ºæµ‹è¯•é›†
    'compression_level': 9,
}

class Interp23Tap(nn.Module):
    """
    PyTorch implementation of the interp23tap MATLAB function.

    Interpolates the input tensor using a 23-coefficient polynomial interpolator,
    upsampling by the given ratio. The ratio must be a power of 2.

    Args:
        ratio (int): Scale ratio for upsampling. Must be a power of 2.
    """

    def __init__(self, ratio: int, pad_mode: str = "replicate"):
        super().__init__()

        if not (ratio > 0 and (ratio & (ratio - 1) == 0)):
            raise ValueError("Error: Only resize factors power of 2 are supported.")
        self.ratio = ratio
        self.num_upsamples = int(math.log2(ratio))
        self.pad_mode = pad_mode

        # Define the 23-tap filter coefficients (CDF23 from MATLAB code)
        cdf23_coeffs = 2.0 * np.array(
            [
                0.5,
                0.305334091185,
                0.0,
                -0.072698593239,
                0.0,
                0.021809577942,
                0.0,
                -0.005192756653,
                0.0,
                0.000807762146,
                0.0,
                -0.000060081482,
            ]
        )
        # Make symmetric
        base_coeffs = np.concatenate([np.flip(cdf23_coeffs[1:]), cdf23_coeffs])
        base_coeffs_t = torch.tensor(base_coeffs, dtype=torch.float32)

        # Reshape kernel for 2D convolution (separable filter)
        # Kernel for filtering along height (columns in MATLAB)
        kernel_h = base_coeffs_t.view(1, 1, -1, 1)  # Shape (1, 1, 23, 1)
        # Kernel for filtering along width (rows in MATLAB)
        kernel_w = base_coeffs_t.view(1, 1, 1, -1)  # Shape (1, 1, 1, 23)

        # Register kernels as buffers
        self.register_buffer("kernel_h", kernel_h)
        self.register_buffer("kernel_w", kernel_w)

        # Calculate padding size (kernel_size=23)
        self.padding = (base_coeffs_t.shape[0] - 1) // 2  # Should be 11

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for interpolation.

        Args:
            x (torch.Tensor): Input tensor of shape (bs, c, h, w).

        Returns:
            torch.Tensor: Interpolated tensor of shape (bs, c, h * ratio, w * ratio).
        """
        if self.ratio == 1:
            return x

        current_img = x
        bs, c, h_curr, w_curr = current_img.shape

        for k in range(self.num_upsamples):
            h_curr *= 2
            w_curr *= 2

            # Upsample by inserting zeros
            upsampled = torch.zeros(
                bs, c, h_curr, w_curr, device=x.device, dtype=x.dtype
            )

            # Place original pixels according to MATLAB logic
            if k == 0:
                # I1LRU(2:2:end,2:2:end,:) = I_Interpolated;
                upsampled[..., 1::2, 1::2] = current_img
            else:
                # I1LRU(1:2:end,1:2:end,:) = I_Interpolated;
                upsampled[..., ::2, ::2] = current_img

            # Apply separable convolution with circular padding
            # Grouped convolution: apply filter independently per channel
            # Using conv2d with groups=c is efficient

            # Pad for horizontal filter (width)
            # Pad width dimension (dim 3) by self.padding on both sides
            padded_w = F.pad(
                upsampled, (self.padding, self.padding, 0, 0), mode=self.pad_mode
            )
            # Apply horizontal filter
            # Input: (bs, c, H, W_padded), Kernel: (1, 1, 1, K) -> Output: (bs, c, H, W)
            # We need kernel shape (c, 1, 1, K) for grouped convolution
            kernel_w_grouped = self.kernel_w.repeat(c, 1, 1, 1)
            filtered_w = F.conv2d(padded_w, kernel_w_grouped, groups=c)

            # Pad for vertical filter (height)
            # Pad height dimension (dim 2) by self.padding on both sides
            padded_h = F.pad(
                filtered_w, (0, 0, self.padding, self.padding), mode="circular"
            )
            # Apply vertical filter
            # Input: (bs, c, H_padded, W), Kernel: (1, 1, K, 1) -> Output: (bs, c, H, W)
            # We need kernel shape (c, 1, K, 1) for grouped convolution
            kernel_h_grouped = self.kernel_h.repeat(c, 1, 1, 1)
            filtered_h = F.conv2d(padded_h, kernel_h_grouped, groups=c)

            current_img = filtered_h  # Update image for next iteration

        return current_img


def anti_aliasing_downsample(image_tensor, factor, device):
    """
    ä½¿ç”¨Interp23Tapè¿›è¡ŒæŠ—æ··å ä¸‹é‡‡æ ·
    
    å®ç°åŸç†ï¼š
    1. ä½¿ç”¨Interp23Tapçš„åå‘è¿‡ç¨‹è¿›è¡ŒæŠ—æ··å æ»¤æ³¢
    2. ç„¶åè¿›è¡Œå­é‡‡æ ·
    
    Args:
        image_tensor: è¾“å…¥å›¾åƒå¼ é‡ (C, H, W)
        factor: ä¸‹é‡‡æ ·å€æ•°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        ä¸‹é‡‡æ ·åçš„å›¾åƒå¼ é‡
    """
    if factor == 1:
        return image_tensor
    
    # æ·»åŠ batchç»´åº¦
    img_batch = image_tensor.unsqueeze(0).to(device)  # (1, C, H, W)
    
    # ä½¿ç”¨Interp23Tapçš„æ»¤æ³¢æ ¸è¿›è¡ŒæŠ—æ··å é¢„å¤„ç†
    # è·å–æ»¤æ³¢æ ¸ç³»æ•°
    cdf23_coeffs = 2.0 * np.array([
        0.5, 0.305334091185, 0.0, -0.072698593239, 0.0, 0.021809577942,
        0.0, -0.005192756653, 0.0, 0.000807762146, 0.0, -0.000060081482,
    ])
    base_coeffs = np.concatenate([np.flip(cdf23_coeffs[1:]), cdf23_coeffs])
    base_coeffs_t = torch.tensor(base_coeffs, dtype=torch.float32, device=device)
    
    # åˆ›å»ºåˆ†ç¦»æ»¤æ³¢æ ¸
    kernel_h = base_coeffs_t.view(1, 1, -1, 1)  # å‚ç›´æ»¤æ³¢æ ¸
    kernel_w = base_coeffs_t.view(1, 1, 1, -1)  # æ°´å¹³æ»¤æ³¢æ ¸
    padding = (len(base_coeffs) - 1) // 2
    
    # åº”ç”¨æŠ—æ··å æ»¤æ³¢
    bs, c, h, w = img_batch.shape
    
    # æ°´å¹³æ»¤æ³¢
    padded_w = F.pad(img_batch, (padding, padding, 0, 0), mode='replicate')
    kernel_w_grouped = kernel_w.repeat(c, 1, 1, 1)
    filtered_w = F.conv2d(padded_w, kernel_w_grouped, groups=c)
    
    # å‚ç›´æ»¤æ³¢
    padded_h = F.pad(filtered_w, (0, 0, padding, padding), mode='replicate')
    kernel_h_grouped = kernel_h.repeat(c, 1, 1, 1)
    filtered_h = F.conv2d(padded_h, kernel_h_grouped, groups=c)
    
    # ä¸‹é‡‡æ ·ï¼ˆå­é‡‡æ ·ï¼‰
    downsampled = filtered_h[:, :, ::factor, ::factor]
    
    return downsampled.squeeze(0)  # ç§»é™¤batchç»´åº¦

def anti_aliasing_downsample_batch(image_tensors, factor, devices):
    """
    å¤šGPUæ‰¹é‡æŠ—æ··å ä¸‹é‡‡æ ·
    
    Args:
        image_tensors: è¾“å…¥å›¾åƒå¼ é‡åˆ—è¡¨
        factor: ä¸‹é‡‡æ ·å€æ•°
        devices: GPUè®¾å¤‡åˆ—è¡¨
        
    Returns:
        ä¸‹é‡‡æ ·åçš„å›¾åƒå¼ é‡åˆ—è¡¨
    """
    if factor == 1:
        return image_tensors
    
    if not devices:
        # å•GPUæˆ–CPUå¤„ç†
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return [anti_aliasing_downsample(img, factor, device) for img in image_tensors]
    
    # å¤šGPUå¹¶è¡Œå¤„ç†
    results = []
    batch_size = len(image_tensors)
    gpu_count = len(devices)
    
    # å°†æ•°æ®åˆ†é…åˆ°ä¸åŒGPU
    for i in range(0, batch_size, gpu_count):
        gpu_results = []
        
        # åˆ›å»ºçº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=gpu_count) as executor:
            futures = []
            
            for j, device_id in enumerate(devices):
                if i + j < batch_size:
                    img_tensor = image_tensors[i + j]
                    device = torch.device(f'cuda:{device_id}')
                    future = executor.submit(anti_aliasing_downsample, img_tensor, factor, device)
                    futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                gpu_results.append(future.result())
        
        results.extend(gpu_results)
    
    return results

class HarvardDatasetGenerator:
    """Harvardæ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(self, config=None, gpu_ids=[0, 1, 2, 3, 4, 5, 6]):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = config or DEFAULT_CONFIG.copy()
        
        # è®¾ç½®å¤šGPUæ”¯æŒ
        self.gpu_ids = gpu_ids if torch.cuda.is_available() else []
        if self.gpu_ids:
            self.device = torch.device(f'cuda:{gpu_ids[0]}')
            print(f"ä½¿ç”¨GPU: {gpu_ids}, ä¸»è®¾å¤‡: {self.device}")
            
            # æ£€æŸ¥GPUå¯ç”¨æ€§
            for gpu_id in gpu_ids:
                if gpu_id >= torch.cuda.device_count():
                    print(f"è­¦å‘Š: GPU {gpu_id} ä¸å¯ç”¨ï¼Œå¿½ç•¥")
                    self.gpu_ids.remove(gpu_id)
            
            print(f"å®é™…ä½¿ç”¨GPU: {self.gpu_ids}")
        else:
            self.device = torch.device('cpu')
            print("ä½¿ç”¨CPU")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
    def load_harvard_data(self):
        """åŠ è½½åŸå§‹Harvardæ•°æ®"""
        print("ğŸ“‚ åŠ è½½åŸå§‹Harvardæ•°æ®...")
        
        harvard_file = self.config['harvard_file']
        if not os.path.exists(harvard_file):
            raise FileNotFoundError(f"Harvardæ–‡ä»¶ä¸å­˜åœ¨: {harvard_file}")
        
        with h5py.File(harvard_file, 'r') as f:
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
            print(f"   æ–‡ä»¶ä¸­çš„æ•°æ®é›†: {list(f.keys())}")
            
            # åŠ è½½æ•°æ®
            self.gt_data = f['gt'][:]          # (77, 31, 1040, 1392)
            self.hrmsi_data = f['HR_MSI'][:]   # (77, 3, 1040, 1392)
            
            # åŠ è½½æ ·æœ¬åç§°
            if 'sample_names' in f:
                sample_names_raw = f['sample_names'][:]
                self.sample_names = [name.decode('utf-8') if isinstance(name, bytes) else str(name) 
                                   for name in sample_names_raw]
            else:
                self.sample_names = [f'harvard_{i:02d}' for i in range(len(self.gt_data))]
        
        print(f"   GTæ•°æ®: {self.gt_data.shape}")
        print(f"   HRMSIæ•°æ®: {self.hrmsi_data.shape}")
        print(f"   æ ·æœ¬æ•°é‡: {len(self.sample_names)}")
        print(f"   æ•°å€¼èŒƒå›´: GT[{self.gt_data.min():.2f}, {self.gt_data.max():.2f}]")
        print(f"   å›¾åƒå°ºå¯¸: {self.gt_data.shape[-2:]} (H x W)")
        
    def crop_patches_overlapping(self, image, patch_size, stride):
        """ä½¿ç”¨é‡å æ»‘åŠ¨çª—å£è£å‰ªpatches"""
        h, w = image.shape[-2:]
        patches = []
        positions = []
        
        for y in range(0, h - patch_size + 1, stride):
            for x in range(0, w - patch_size + 1, stride):
                if len(image.shape) == 3:  # (C, H, W)
                    patch = image[:, y:y+patch_size, x:x+patch_size]
                else:  # (H, W)
                    patch = image[y:y+patch_size, x:x+patch_size]
                patches.append(patch)
                positions.append((y, x))
        
        return patches, positions
    
    def crop_test_region(self, image, crop_size):
        """è£å‰ªæµ‹è¯•é›†åŒºåŸŸï¼ˆå·¦ä¸Šè§’ï¼‰"""
        if len(image.shape) == 3:  # (C, H, W)
            return image[:, :crop_size, :crop_size]
        else:  # (H, W)
            return image[:crop_size, :crop_size]
    
    def generate_lrhsi_and_lms(self, images, downsample_factors, original_size=None):
        """
        ç”ŸæˆLRHSIå’ŒLMSæ•°æ® - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        """
        lrhsi_data = {}
        lms_data = {}
        
        for factor in downsample_factors:
            lrhsi_data[f'LRHSI_{factor}'] = []
            lms_data[f'lms_{factor}'] = []
        
        print(f"ğŸ”„ ç”ŸæˆLRHSIå’ŒLMSæ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...")
        
        # å¤§å¹…å‡å°‘æ‰¹å¤„ç†å¤§å°ä»¥èŠ‚çœå†…å­˜
        batch_size = 2 if self.gpu_ids else 1
        
        for factor in downsample_factors:
            print(f"   å¤„ç†ä¸‹é‡‡æ ·å€æ•° {factor}x...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹å¤„ç†å®Œç«‹å³æ¸…ç†å†…å­˜
            for i in range(0, len(images), batch_size):
                batch_images = images[i:i+batch_size]
                
                try:
                    # å¤„ç†å½“å‰æ‰¹æ¬¡
                    for j, image in enumerate(batch_images):
                        # è½¬æ¢ä¸ºtensor
                        if isinstance(image, np.ndarray):
                            img_tensor = torch.from_numpy(image).float()
                        else:
                            img_tensor = image
                        
                        # ä¸‹é‡‡æ ·ï¼ˆå•å›¾åƒå¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
                        lrhsi_tensor = anti_aliasing_downsample(img_tensor, factor, self.device)
                        
                        # ç§»åŠ¨åˆ°CPUå¹¶ä¿å­˜LRHSI
                        lrhsi = lrhsi_tensor.cpu().numpy()
                        lrhsi_data[f'LRHSI_{factor}'].append(lrhsi)
                        
                        # ç”ŸæˆLMSï¼ˆä¸Šé‡‡æ ·ï¼‰
                        target_size = original_size if original_size is not None else image.shape[-2:]
                        lrhsi_tensor = lrhsi_tensor.to(self.device)
                        lrhsi_batch = lrhsi_tensor.unsqueeze(0)
                        lms_tensor = F.interpolate(lrhsi_batch, size=target_size, 
                                                 mode='bilinear', align_corners=False)
                        lms = lms_tensor.squeeze(0).cpu().numpy()
                        lms_data[f'lms_{factor}'].append(lms)
                        
                        # ç«‹å³æ¸…ç†GPUå†…å­˜
                        del img_tensor, lrhsi_tensor, lrhsi_batch, lms_tensor
                        if self.device.type == 'cuda':
                            torch.cuda.empty_cache()
                        
                        # å®šæœŸå¼ºåˆ¶åƒåœ¾å›æ”¶
                        if (i + j + 1) % 10 == 0:
                            import gc
                            gc.collect()
                            
                except Exception as e:
                    print(f"æ‰¹å¤„ç† {i//batch_size} å¤±è´¥ (factor={factor}): {e}")
                    # é™çº§åˆ°æœ€ç®€å•çš„å¤„ç†æ–¹å¼
                    for image in batch_images:
                        try:
                            # ä½¿ç”¨numpyè¿›è¡Œç®€å•ä¸‹é‡‡æ ·ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                            if factor > 1:
                                lrhsi = image[:, ::factor, ::factor]
                            else:
                                lrhsi = image
                            lrhsi_data[f'LRHSI_{factor}'].append(lrhsi)
                            
                            # ç®€å•ä¸Šé‡‡æ ·
                            from scipy.ndimage import zoom
                            if original_size is not None:
                                zoom_factors = (1, original_size[0]/lrhsi.shape[-2], original_size[1]/lrhsi.shape[-1])
                                lms = zoom(lrhsi, zoom_factors, order=1)
                            else:
                                lms = lrhsi
                            lms_data[f'lms_{factor}'].append(lms)
                        except Exception as e2:
                            print(f"åå¤‡å¤„ç†ä¹Ÿå¤±è´¥: {e2}")
                
                # æ¯æ‰¹å¤„ç†å®Œåæ˜¾ç¤ºè¿›åº¦å’Œå†…å­˜ä½¿ç”¨
                processed = min(i + batch_size, len(images))
                print(f"     è¿›åº¦: {processed}/{len(images)} ({100*processed/len(images):.1f}%)")
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
        
        return lrhsi_data, lms_data
    
    def generate_training_set(self):
        """ç”Ÿæˆè®­ç»ƒé›† - çœŸæ­£çš„è¾¹å­˜è¾¹å¤„ç†ç‰ˆæœ¬"""
        print("\n" + "="*60)
        print("1ï¸âƒ£ ç”ŸæˆHarvardè®­ç»ƒé›†ï¼ˆè¾¹å­˜è¾¹å¤„ç†ï¼‰")
        print("="*60)
        
        # æå–è®­ç»ƒæ•°æ®
        train_indices = self.config['train_indices']
        patch_size = self.config['patch_size']
        stride = self.config['stride']
        downsample_factors = self.config['downsample_factors']
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_indices)}å¼ å›¾åƒ")
        
        # ä¼°ç®—patchesæ•°é‡
        sample_gt = self.gt_data[train_indices[0]]
        sample_patches, _ = self.crop_patches_overlapping(sample_gt, patch_size, stride)
        patches_per_image = len(sample_patches)
        total_estimated_patches = patches_per_image * len(train_indices)
        
        print(f"æ¯å¼ å›¾åƒä¼°è®¡patches: {patches_per_image}")
        print(f"æ€»ä¼°è®¡patches: {total_estimated_patches}")
        
        # åˆ›å»ºè¾“å‡ºH5æ–‡ä»¶å¹¶é¢„åˆ†é…ç©ºé—´
        train_file = os.path.join(
            self.config['output_dir'], 
            f'Harvard_train_patches_stride{stride}_size{patch_size}.h5'
        )
        
        compression_opts = self.config['compression_level']
        
        # æ‰“å¼€H5æ–‡ä»¶å¹¶é¢„åˆ†é…æ•°æ®é›†
        with h5py.File(train_file, 'w') as f:
            # é¢„åˆ†é…æ‰€æœ‰æ•°æ®é›†
            gt_dataset = f.create_dataset(
                'GT', 
                shape=(total_estimated_patches, 31, patch_size, patch_size),
                dtype=np.float32,
                # compression='gzip', 
                # compression_opts=compression_opts,
                # chunks=True  # å¯ç”¨åˆ†å—å­˜å‚¨
            )
            
            hrmsi_dataset = f.create_dataset(
                'HRMSI', 
                shape=(total_estimated_patches, 3, patch_size, patch_size),
                dtype=np.float32,
                # compression='gzip', 
                # compression_opts=compression_opts,
                # chunks=True
            )
            
            # ä¸ºæ¯ä¸ªä¸‹é‡‡æ ·å€æ•°é¢„åˆ†é…æ•°æ®é›†
            lrhsi_datasets = {}
            lms_datasets = {}
            
            for factor in downsample_factors:
                lrhsi_size = patch_size // factor
                
                lrhsi_datasets[factor] = f.create_dataset(
                    f'LRHSI_{factor}',
                    shape=(total_estimated_patches, 31, lrhsi_size, lrhsi_size),
                    dtype=np.float32,
                    # compression='gzip',
                    # compression_opts=compression_opts,
                    # chunks=True
                )
                
                lms_datasets[factor] = f.create_dataset(
                    f'lms_{factor}',
                    shape=(total_estimated_patches, 31, patch_size, patch_size),
                    dtype=np.float32,
                    # compression='gzip',
                    # compression_opts=compression_opts,
                    # chunks=True
                )
        
            # é€å›¾åƒå¤„ç†å¹¶ç›´æ¥å†™å…¥H5æ–‡ä»¶
            print("\nğŸ“¦ é€å›¾åƒè¾¹å­˜è¾¹å¤„ç†...")
            current_patch_idx = 0
            
            for img_idx, idx in enumerate(train_indices):
                print(f"å¤„ç†å›¾åƒ {img_idx+1}/{len(train_indices)}: {self.sample_names[idx]}")
                
                # åŠ è½½å•å¼ å›¾åƒ
                gt_image = self.gt_data[idx]      # (31, 1040, 1392)
                hrmsi_image = self.hrmsi_data[idx]  # (3, 1040, 1392)
                
                # è£å‰ªpatches
                gt_patches, _ = self.crop_patches_overlapping(gt_image, patch_size, stride)
                hrmsi_patches, _ = self.crop_patches_overlapping(hrmsi_image, patch_size, stride)
                
                # é€patchå¤„ç†å¹¶å†™å…¥
                patch_count = len(gt_patches)
                
                # åˆ†å°æ‰¹æ¬¡å¤„ç†patchesä»¥æ§åˆ¶å†…å­˜
                batch_size = 100  # æ¯æ¬¡å¤„ç†20ä¸ªpatches
                
                for batch_start in range(0, patch_count, batch_size):
                    batch_end = min(batch_start + batch_size, patch_count)
                    batch_patches = gt_patches[batch_start:batch_end]
                    
                    # æ‰¹é‡ç”ŸæˆLRHSIå’ŒLMS
                    batch_lrhsi, batch_lms = self.generate_lrhsi_and_lms(
                        batch_patches, downsample_factors, 
                        original_size=(patch_size, patch_size)
                    )
                    
                    # å†™å…¥å½“å‰æ‰¹æ¬¡åˆ°H5æ–‡ä»¶
                    start_idx = current_patch_idx + batch_start
                    end_idx = current_patch_idx + batch_end
                    
                    # å†™å…¥GTå’ŒHRMSI
                    gt_dataset[start_idx:end_idx] = np.stack(batch_patches)
                    hrmsi_dataset[start_idx:end_idx] = np.stack(hrmsi_patches[batch_start:batch_end])
                    
                    # å†™å…¥LRHSIå’ŒLMS
                    for factor in downsample_factors:
                        lrhsi_datasets[factor][start_idx:end_idx] = np.stack(batch_lrhsi[f'LRHSI_{factor}'])
                        lms_datasets[factor][start_idx:end_idx] = np.stack(batch_lms[f'lms_{factor}'])
                
                # æ›´æ–°patchç´¢å¼•
                current_patch_idx += patch_count
                
                # æ¸…ç†å›¾åƒæ•°æ®
                del gt_image, hrmsi_image, gt_patches, hrmsi_patches
                
                # å®šæœŸåƒåœ¾å›æ”¶
                if (img_idx + 1) % 5 == 0:
                    import gc
                    gc.collect()
                    if self.device.type == 'cuda':
                            torch.cuda.empty_cache()
            
            # å¦‚æœå®é™…patchesæ•°å°‘äºä¼°è®¡æ•°ï¼Œè°ƒæ•´æ•°æ®é›†å¤§å°
            if current_patch_idx < total_estimated_patches:
                print(f"\nğŸ“ è°ƒæ•´æ•°æ®é›†å¤§å°: {total_estimated_patches} -> {current_patch_idx}")
                
                # è°ƒæ•´æ‰€æœ‰æ•°æ®é›†çš„å¤§å°
                gt_dataset.resize((current_patch_idx, 31, patch_size, patch_size))
                hrmsi_dataset.resize((current_patch_idx, 3, patch_size, patch_size))
                
                for factor in downsample_factors:
                    lrhsi_size = patch_size // factor
                    lrhsi_datasets[factor].resize((current_patch_idx, 31, lrhsi_size, lrhsi_size))
                    lms_datasets[factor].resize((current_patch_idx, 31, patch_size, patch_size))
            
            # ä¿å­˜å…ƒæ•°æ® - ç§»åˆ°withè¯­å¥å†…éƒ¨
            f.attrs['patch_size'] = patch_size
            f.attrs['stride'] = stride
            f.attrs['total_patches'] = current_patch_idx
            f.attrs['downsample_factors'] = downsample_factors
            f.attrs['train_images'] = len(train_indices)
            f.attrs['original_image_size'] = list(self.gt_data.shape[-2:])
    
        # è¿™äº›ä»£ç åœ¨withè¯­å¥å¤–éƒ¨æ˜¯å®‰å…¨çš„
        file_size_mb = os.path.getsize(train_file) / (1024**2)
        print(f"âœ… è®­ç»ƒé›†ä¿å­˜å®Œæˆ:")
        print(f"   æ–‡ä»¶: {train_file}")
        print(f"   å¤§å°: {file_size_mb:.1f} MB")
        print(f"   å®é™…patches: {current_patch_idx}")

        return train_file

    def generate_test_set(self):
        """ç”Ÿæˆæµ‹è¯•é›† - è¾¹å­˜è¾¹å¤„ç†ç‰ˆæœ¬"""
        print("\n" + "="*60)
        print("2ï¸âƒ£ ç”ŸæˆHarvardæµ‹è¯•é›†ï¼ˆè¾¹å­˜è¾¹å¤„ç†ï¼‰")
        print("="*60)
        
        test_indices = self.config['test_indices']
        test_names = [self.sample_names[i] for i in test_indices]
        crop_size = self.config['test_crop_size']
        downsample_factors = self.config['downsample_factors']
        
        print(f"æµ‹è¯•æ ·æœ¬: {test_names}")
        print(f"è£å‰ªå°ºå¯¸: {crop_size}x{crop_size}")
        
        # åˆ›å»ºè¾“å‡ºH5æ–‡ä»¶
        test_file = os.path.join(self.config['output_dir'], f'Harvard_test_crop{crop_size}.h5')
        compression_opts = self.config['compression_level']
        
        with h5py.File(test_file, 'w') as f:
            num_test_images = len(test_indices)
            
            # é¢„åˆ†é…æµ‹è¯•é›†æ•°æ®é›†
            gt_dataset = f.create_dataset(
                'GT', 
                shape=(num_test_images, 31, crop_size, crop_size),
                dtype=np.float32,
                # compression='gzip', 
                # compression_opts=compression_opts,
                # chunks=True
            )
            
            hrmsi_dataset = f.create_dataset(
                'HRMSI', 
                shape=(num_test_images, 3, crop_size, crop_size),
                dtype=np.float32,
                # compression='gzip', 
                # compression_opts=compression_opts,
                # chunks=True
            )
            
            # ä¸ºæ¯ä¸ªä¸‹é‡‡æ ·å€æ•°é¢„åˆ†é…æ•°æ®é›†
            lrhsi_datasets = {}
            lms_datasets = {}
            
            for factor in downsample_factors:
                lrhsi_size = crop_size // factor
                
                lrhsi_datasets[factor] = f.create_dataset(
                    f'LRHSI_{factor}',
                    shape=(num_test_images, 31, lrhsi_size, lrhsi_size),
                    dtype=np.float32,
                    # compression='gzip',
                    # compression_opts=compression_opts,
                    # chunks=True
                )
                
                lms_datasets[factor] = f.create_dataset(
                    f'lms_{factor}',
                    shape=(num_test_images, 31, crop_size, crop_size),
                    dtype=np.float32,
                    # compression='gzip',
                    # compression_opts=compression_opts,
                    # chunks=True
                )
            
            # é€å›¾åƒå¤„ç†å¹¶ç›´æ¥å†™å…¥
            print("\nğŸ“¦ é€å›¾åƒè¾¹å­˜è¾¹å¤„ç†...")
            
            for i, idx in enumerate(test_indices):
                print(f"å¤„ç†æµ‹è¯•å›¾åƒ {i+1}/{num_test_images}: {test_names[i]}")
                
                # åŠ è½½å¹¶è£å‰ªå›¾åƒ
                gt_full = self.gt_data[idx]
                hrmsi_full = self.hrmsi_data[idx]
                
                gt_cropped = self.crop_test_region(gt_full, crop_size)
                hrmsi_cropped = self.crop_test_region(hrmsi_full, crop_size)
                
                # ç›´æ¥å†™å…¥GTå’ŒHRMSI
                gt_dataset[i] = gt_cropped
                hrmsi_dataset[i] = hrmsi_cropped
                
                # ç”Ÿæˆå¹¶å†™å…¥LRHSIå’ŒLMS
                single_lrhsi, single_lms = self.generate_lrhsi_and_lms(
                    [gt_cropped], downsample_factors, original_size=(crop_size, crop_size))
                
                for factor in downsample_factors:
                    lrhsi_datasets[factor][i] = single_lrhsi[f'LRHSI_{factor}'][0]
                    lms_datasets[factor][i] = single_lms[f'lms_{factor}'][0]
                
                # ç«‹å³æ¸…ç†å†…å­˜
                del gt_full, hrmsi_full, gt_cropped, hrmsi_cropped, single_lrhsi, single_lms
                
                # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜
                f.flush()
                
                # åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
            
            # ä¿å­˜å›¾åƒåç§°å’Œå…ƒæ•°æ®
            dt = h5py.special_dtype(vlen=str)
            f.create_dataset('image_names', data=test_names, dtype=dt)
            
            f.attrs['total_test_images'] = num_test_images
            f.attrs['crop_size'] = crop_size
            f.attrs['original_image_size'] = list(self.gt_data.shape[-2:])
            f.attrs['downsample_factors'] = downsample_factors
        
        file_size_mb = os.path.getsize(test_file) / (1024**2)
        print(f"âœ… æµ‹è¯•é›†ä¿å­˜å®Œæˆ:")
        print(f"   æ–‡ä»¶: {test_file}")
        print(f"   å¤§å°: {file_size_mb:.1f} MB")
        
        return test_file
    
    def verify_datasets(self, train_file, test_file):
        """éªŒè¯ç”Ÿæˆçš„æ•°æ®é›†"""
        print("\n" + "="*60)
        print("3ï¸âƒ£ éªŒè¯æ•°æ®é›†")
        print("="*60)
        
        # éªŒè¯è®­ç»ƒé›†
        print("ğŸ” éªŒè¯è®­ç»ƒé›†:")
        with h5py.File(train_file, 'r') as f:
            print(f"   æ•°æ®é›†é”®å€¼: {list(f.keys())}")
            for key in ['GT', 'HRMSI'] + [f'LRHSI_{factor}' for factor in self.config['downsample_factors']] + [f'lms_{factor}' for factor in self.config['downsample_factors']]:
                if key in f:
                    data = f[key]
                    print(f"   {key}: {data.shape}, dtype={data.dtype}")
            
            print(f"   å±æ€§: {dict(f.attrs)}")
        
        # éªŒè¯æµ‹è¯•é›†
        print("\nğŸ” éªŒè¯æµ‹è¯•é›†:")
        with h5py.File(test_file, 'r') as f:
            print(f"   æ•°æ®é›†é”®å€¼: {list(f.keys())}")
            for key in ['GT', 'HRMSI'] + [f'LRHSI_{factor}' for factor in self.config['downsample_factors']] + [f'lms_{factor}' for factor in self.config['downsample_factors']]:
                if key in f:
                    data = f[key]
                    print(f"   {key}: {data.shape}, dtype={data.dtype}")
            
            if 'image_names' in f:
                names = [name.decode('utf-8') if isinstance(name, bytes) else name for name in f['image_names'][:]]
                print(f"   å›¾åƒåç§°: {names}")
                
            print(f"   å±æ€§: {dict(f.attrs)}")
        
        print("\nâœ… æ•°æ®é›†éªŒè¯å®Œæˆ!")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹ç”ŸæˆHarvardæ•°æ®é›†")
        print("="*80)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_harvard_data()
            
            # 2. ç”Ÿæˆè®­ç»ƒé›†
            train_file = self.generate_training_set()
            
            # 3. ç”Ÿæˆæµ‹è¯•é›†
            test_file = self.generate_test_set()
            
            # 4. éªŒè¯æ•°æ®é›†
            self.verify_datasets(train_file, test_file)
            
            # 5. æ€»ç»“
            print("\n" + "="*80)
            print("ğŸ‰ Harvardæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
            print("="*80)
            print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            print(f"   è®­ç»ƒé›†: {train_file}")
            print(f"   æµ‹è¯•é›†: {test_file}")
            print()
            print("âœ… æ•°æ®é›†ç‰¹ç‚¹:")
            print("   â€¢ GT: 31é€šé“é«˜å…‰è°±æ•°æ®")
            print("   â€¢ HRMSI: 3é€šé“RGBæ•°æ®")
            print("   â€¢ LRHSI_X: ä½¿ç”¨Interp23TapæŠ—æ··å ä¸‹é‡‡æ ·çš„ä½åˆ†è¾¨ç‡é«˜å…‰è°±æ•°æ®")
            print("   â€¢ lms_X: ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·å›åŸå°ºå¯¸çš„ä½åˆ†è¾¨ç‡å¤šå…‰è°±æ•°æ®")
            print("   â€¢ è®­ç»ƒé›†: 67å¼ å›¾åƒï¼Œ64æ­¥é•¿128x128 patches")
            print("   â€¢ æµ‹è¯•é›†: 10å¼ å›¾åƒï¼Œå·¦ä¸Šè§’1000x1000è£å‰ª")
            print("="*80)
            
            return train_file, test_file
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Harvardæ•°æ®é›†ç”Ÿæˆå™¨')
    
    parser.add_argument('--harvard_file', type=str, 
                       default=DEFAULT_CONFIG['harvard_file'],
                       help='HarvardåŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--output_dir', type=str,
                       default=DEFAULT_CONFIG['output_dir'],
                       help='è¾“å‡ºç›®å½•')
    
    parser.add_argument('--patch_size', type=int,
                       default=DEFAULT_CONFIG['patch_size'],
                       help='patchå°ºå¯¸')
    
    parser.add_argument('--stride', type=int,
                       default=DEFAULT_CONFIG['stride'],
                       help='patchæ­¥é•¿')
    
    parser.add_argument('--test_crop_size', type=int,
                       default=DEFAULT_CONFIG['test_crop_size'],
                       help='æµ‹è¯•é›†è£å‰ªå°ºå¯¸')
    
    parser.add_argument('--downsample_factors', nargs='+', type=int,
                       default=DEFAULT_CONFIG['downsample_factors'],
                       help='ä¸‹é‡‡æ ·å€æ•°åˆ—è¡¨')
    
    parser.add_argument('--compression_level', type=int,
                       default=DEFAULT_CONFIG['compression_level'],
                       help='HDF5å‹ç¼©çº§åˆ« (0-9)')
    
    # æ–°å¢GPUå‚æ•°
    parser.add_argument('--gpu_ids', nargs='+', type=int,
                       default=[0, 1, 2, 3, 4, 5, 6],
                       help='ä½¿ç”¨çš„GPU IDåˆ—è¡¨')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # åˆ›å»ºé…ç½®
    config = DEFAULT_CONFIG.copy()
    gpu_ids = args.gpu_ids
    del args.gpu_ids  # ä»argsä¸­ç§»é™¤gpu_ids
    config.update(vars(args))
    
    print("é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print(f"  gpu_ids: {gpu_ids}")
    print()
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è¿è¡Œ
    generator = HarvardDatasetGenerator(config, gpu_ids=gpu_ids)
    train_file, test_file = generator.run()
    
    if train_file and test_file:
        print(f"\nâœ… æ•°æ®é›†ç”ŸæˆæˆåŠŸ!")
    else:
        print(f"\nâŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥!")
        sys.exit(1)

if __name__ == '__main__':
    main()
