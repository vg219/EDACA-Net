#!/usr/bin/env python3
"""
CAVEæ•°æ®é›†å¿«é€Ÿç”Ÿæˆè„šæœ¬ - ç®€åŒ–ç‰ˆæœ¬
ä¸“æ³¨äºå¿«é€Ÿç”Ÿæˆæ ‡å‡†æ ¼å¼çš„CAVEæ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python generate_cave_simple.py

è¾“å‡º:
    - CAVE_train_patches_stride40_size128.h5  (è®­ç»ƒé›†)
    - CAVE_test_fullsize.h5                   (æµ‹è¯•é›†)
"""

import os
import numpy as np
import h5py
from tqdm import tqdm
import torch
import torch.nn.functional as F

def main():
    print("ğŸš€ CAVEæ•°æ®é›†å¿«é€Ÿç”Ÿæˆå™¨")
    print("="*50)
    
    # é…ç½®
    cave_file = '/data2/users/yujieliang/dataset/CAVE/CAVE_processed.h5'
    output_dir = '/data2/users/yujieliang/dataset'
    patch_size = 128
    stride = 40
    downsample_factors = [4, 8, 16, 32]
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(cave_file):
        print(f"âŒ æ‰¾ä¸åˆ°CAVEæ•°æ®æ–‡ä»¶: {cave_file}")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {cave_file}")
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    with h5py.File(cave_file, 'r') as f:
        gt_data = f['gt'][:]        # (32, 31, 512, 512)
        hrmsi_data = f['HR_MSI'][:]  # (32, 3, 512, 512)
        sample_names = [name.decode('utf-8') if isinstance(name, bytes) else name 
                       for name in f['sample_names'][:]]
    
    print(f"   GT: {gt_data.shape}")
    print(f"   HRMSI: {hrmsi_data.shape}")
    
    # 2. æ•°æ®åˆ†å‰²
    train_indices = list(range(10, 32))  # å22å¼ ä½œä¸ºè®­ç»ƒé›†
    test_indices = list(range(10))       # å‰10å¼ ä½œä¸ºæµ‹è¯•é›†
    
    print(f"   è®­ç»ƒé›†: {len(train_indices)} å¼ å›¾åƒ")
    print(f"   æµ‹è¯•é›†: {len(test_indices)} å¼ å›¾åƒ")
    
    def crop_patches(image, patch_size, stride):
        """è£å‰ªpatches"""
        h, w = image.shape[-2:]
        patches = []
        for y in range(0, h - patch_size + 1, stride):
            for x in range(0, w - patch_size + 1, stride):
                if len(image.shape) == 3:
                    patch = image[:, y:y+patch_size, x:x+patch_size]
                else:
                    patch = image[y:y+patch_size, x:x+patch_size]
                patches.append(patch)
        return patches
    
    def generate_lrhsi_lms(images, factors):
        """ç”ŸæˆLRHSIå’ŒLMSæ•°æ®"""
        lrhsi_data = {}
        lms_data = {}
        
        for factor in factors:
            lrhsi_data[f'LRHSI_{factor}'] = []
            lms_data[f'lms_{factor}'] = []
        
        for factor in factors:
            print(f"   å¤„ç†factor {factor}...")
            for img in tqdm(images, desc=f"Factor {factor}"):
                # è½¬æ¢ä¸ºtensor
                img_tensor = torch.from_numpy(img).float().unsqueeze(0)  # (1, C, H, W)
                
                # ä¸‹é‡‡æ ·
                lrhsi_tensor = F.avg_pool2d(img_tensor, kernel_size=factor, stride=factor)
                lrhsi = lrhsi_tensor.squeeze(0).numpy()
                lrhsi_data[f'LRHSI_{factor}'].append(lrhsi)
                
                # ä¸Šé‡‡æ ·
                original_size = img.shape[-2:]
                lms_tensor = F.interpolate(lrhsi_tensor, size=original_size, 
                                         mode='bilinear', align_corners=False)
                lms = lms_tensor.squeeze(0).numpy()
                lms_data[f'lms_{factor}'].append(lms)
        
        return lrhsi_data, lms_data
    
    # 3. ç”Ÿæˆè®­ç»ƒé›†
    print("\nğŸ“¦ ç”Ÿæˆè®­ç»ƒé›†...")
    
    train_gt = gt_data[train_indices]
    train_hrmsi = hrmsi_data[train_indices]
    
    # è£å‰ªpatches
    print("   è£å‰ªpatches...")
    all_gt_patches = []
    all_hrmsi_patches = []
    
    for i in tqdm(range(len(train_gt)), desc="è£å‰ª"):
        gt_patches = crop_patches(train_gt[i], patch_size, stride)
        hrmsi_patches = crop_patches(train_hrmsi[i], patch_size, stride)
        all_gt_patches.extend(gt_patches)
        all_hrmsi_patches.extend(hrmsi_patches)
    
    print(f"   æ€»patches: {len(all_gt_patches)}")
    
    # ç”ŸæˆLRHSIå’ŒLMS
    print("   ç”ŸæˆLRHSIå’ŒLMS...")
    train_lrhsi, train_lms = generate_lrhsi_lms(all_gt_patches, downsample_factors)
    
    # ä¿å­˜è®­ç»ƒé›†
    print("   ä¿å­˜è®­ç»ƒé›†...")
    train_file = os.path.join(output_dir, f'CAVE_train_patches_stride{stride}_size{patch_size}.h5')
    
    with h5py.File(train_file, 'w') as f:
        # ä¿å­˜æ•°æ®
        f.create_dataset('GT', data=np.stack(all_gt_patches), compression='gzip', compression_opts=9)
        f.create_dataset('HRMSI', data=np.stack(all_hrmsi_patches), compression='gzip', compression_opts=9)
        
        for factor in downsample_factors:
            f.create_dataset(f'LRHSI_{factor}', data=np.stack(train_lrhsi[f'LRHSI_{factor}']), 
                           compression='gzip', compression_opts=9)
            f.create_dataset(f'lms_{factor}', data=np.stack(train_lms[f'lms_{factor}']), 
                           compression='gzip', compression_opts=9)
        
        # ä¿å­˜å…ƒæ•°æ®
        f.attrs['patch_size'] = patch_size
        f.attrs['stride'] = stride
        f.attrs['total_patches'] = len(all_gt_patches)
        f.attrs['downsample_factors'] = downsample_factors
    
    train_size = os.path.getsize(train_file) / (1024**2)
    print(f"   âœ… è®­ç»ƒé›†: {train_file} ({train_size:.1f} MB)")
    
    # 4. ç”Ÿæˆæµ‹è¯•é›†
    print("\nğŸ“¦ ç”Ÿæˆæµ‹è¯•é›†...")
    
    test_gt = gt_data[test_indices]
    test_hrmsi = hrmsi_data[test_indices]
    test_names = [sample_names[i] for i in test_indices]
    
    # ç”ŸæˆLRHSIå’ŒLMS
    print("   ç”Ÿæˆæµ‹è¯•é›†LRHSIå’ŒLMS...")
    test_gt_list = [test_gt[i] for i in range(len(test_gt))]
    test_lrhsi, test_lms = generate_lrhsi_lms(test_gt_list, downsample_factors)
    
    # ä¿å­˜æµ‹è¯•é›†
    print("   ä¿å­˜æµ‹è¯•é›†...")
    test_file = os.path.join(output_dir, 'CAVE_test_fullsize.h5')
    
    with h5py.File(test_file, 'w') as f:
        # ä¿å­˜æ•°æ®
        f.create_dataset('GT', data=test_gt, compression='gzip', compression_opts=9)
        f.create_dataset('HRMSI', data=test_hrmsi, compression='gzip', compression_opts=9)
        
        for factor in downsample_factors:
            f.create_dataset(f'LRHSI_{factor}', data=np.stack(test_lrhsi[f'LRHSI_{factor}']), 
                           compression='gzip', compression_opts=9)
            f.create_dataset(f'lms_{factor}', data=np.stack(test_lms[f'lms_{factor}']), 
                           compression='gzip', compression_opts=9)
        
        # ä¿å­˜å›¾åƒåç§°
        dt = h5py.special_dtype(vlen=str)
        f.create_dataset('image_names', data=test_names, dtype=dt)
        
        # ä¿å­˜å…ƒæ•°æ®
        f.attrs['total_test_images'] = len(test_indices)
        f.attrs['image_size'] = [512, 512]
        f.attrs['downsample_factors'] = downsample_factors
    
    test_size = os.path.getsize(test_file) / (1024**2)
    print(f"   âœ… æµ‹è¯•é›†: {test_file} ({test_size:.1f} MB)")
    
    # 5. éªŒè¯æ•°æ®
    print("\nğŸ” éªŒè¯æ•°æ®é›†...")
    
    with h5py.File(train_file, 'r') as f:
        print(f"   è®­ç»ƒé›†é”®å€¼: {list(f.keys())}")
        print(f"   GT shape: {f['GT'].shape}")
        print(f"   LRHSI_4 shape: {f['LRHSI_4'].shape}")
        print(f"   lms_4 shape: {f['lms_4'].shape}")
    
    with h5py.File(test_file, 'r') as f:
        print(f"   æµ‹è¯•é›†é”®å€¼: {list(f.keys())}")
        print(f"   GT shape: {f['GT'].shape}")
        print(f"   æµ‹è¯•å›¾åƒ: {[name.decode('utf-8') for name in f['image_names'][:3]]}...")
    
    print("\n" + "="*50)
    print("ğŸ‰ CAVEæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
    print("="*50)
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   è®­ç»ƒé›†: {train_file}")
    print(f"   æµ‹è¯•é›†: {test_file}")
    print()
    print("âœ… æ•°æ®ç»“æ„:")
    print("   GT - 31é€šé“é«˜å…‰è°±æ•°æ®")
    print("   HRMSI - 3é€šé“RGBæ•°æ®")
    print("   LRHSI_4/8/16/32 - ä¸‹é‡‡æ ·çš„ä½åˆ†è¾¨ç‡é«˜å…‰è°±")
    print("   lms_4/8/16/32 - ä¸Šé‡‡æ ·çš„ä½åˆ†è¾¨ç‡å¤šå…‰è°±")
    print("="*50)

if __name__ == '__main__':
    main()
