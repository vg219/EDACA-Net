datasets_cfg:
  vae:
    root_of_dirs: "/Data3/cao/ZiHanCao/datasets"
    aug_prob: 0.5

# vae cfg
vae_cfg:
  pretrained_ckpt: "model/lfq_vae/ckpts/imagenet_256_L.ckpt"

  encoder_cfg:
    ch: 128
    in_channels: 3
    num_res_blocks: 4
    z_channels: 18
    ch_mult: [1, 1, 2, 2, 4]
    resolution: 128
    with_checkpoint: true

  decoder_cfg:
    ch: 128
    in_channels: 3
    out_ch: 3
    num_res_blocks: 4
    z_channels: 18
    ch_mult: [1, 1, 2, 2, 4]
    resolution: 128
    with_checkpoint: true

  lfq_cfg:
    dim: 18
    codebook_size: 262144
    # channel_first: true
    # spherical: true

  # discriminator cfg
  vq_loss_cfg:
    disc_in_channels: 3
    use_actnorm: false
    disc_start: 0
    disc_weight: 0.8
    gen_loss_weight: 0.1
    lecam_loss_weight: 0.05
    codebook_weight: 0.1  # hacked in lfq, not in lfq_v0
    commit_weight: 0.25  # hacked in lfq, not in lfq_v0
    codebook_enlarge_ratio: 0
    codebook_enlarge_steps: 2000
    loss_ssim: true

# ema_cfg
ema_cfg:
  beta: 0.999
  update_after_step: 100
  update_every: 1

# optimizer cfg
optimizer_cfg:
  gradient_accumulation_steps: 1
  gan_opt:
    # name: adam
    # lr: 1.0e-4
    # weight_decay: 1.0e-6
    # betas: [0.5, 0.9]

    name: soap
    lr: 5.0e-5
    weight_decay: 1.0e-4
    # betas: [0.5, 0.9]
    eps: 1.0e-8
    precondition_frequency: 2

  disc_opt:
    # name: adam
    # lr: 1.0e-4
    # weight_decay: 1.0e-6
    # betas: [0.5, 0.9]

    name: soap
    lr: 5.0e-5
    weight_decay: 1.0e-4
    # betas: [0.5, 0.9]
    eps: 1.0e-8
    precondition_frequency: 2

  gan_lr_sched:
    name: cos_anneal_restart_reduce
    T_0: 2000
    T_mult: 2
    eta_min: 5.0e-6
    # warmup_epochs: 1000  # actually, steps here

    # name: constant
    
  disc_lr_sched:
    name: cos_anneal_restart_reduce
    T_0: 2000
    T_mult: 2
    eta_min: 5.0e-6
    # warmup_epochs: 1000  # actually, steps here

    # name: constant

# train cfg
train_cfg:
  save_state_every: 1000
  val_every: 1000
  opt_disc_n_steps: 1
  train_gen_after_n_iters: 0
  save_train_every: 200
  max_train_steps: 100_000

save_checker_cfg:
  metric_name: psnr
  check_order: up

# dataloader cfg
train_bs: 8
num_workers: 2
shuffle: yes

# logger cfg
logger_config:
  name: lfq_vae
  base_path: log_file/
  file_mode: w
  log_every: 5
  